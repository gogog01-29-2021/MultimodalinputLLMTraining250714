# Natural Video LLM

This project aims to train an LLM using real-time natural human facial expressions captured from live video during interaction, without using any symbolic labels like "AU" or "emotion class". The goal is to build a conversational model that learns to adapt and align responses subconsciously, the way humans co-regulate tone and meaning in real life.

## Project Structure

```
natural_video_llm/
├── capture/
│   ├── webcam_video_logger.py      # records video + timestamp
│   ├── text_sync_logger.py         # records chat text + timestamps
│   └── session.jsonl               # time-aligned (video, text) logs
├── preprocessing/
│   ├── extract_video_latents.py    # ViT/TimeSformer encoder
│   ├── align_text_video.py         # token-level alignment
├── models/
│   ├── fusion_transformer.py       # LLM + cross-modal adapter
│   ├── loss.py                     # contrastive / fusion loss
├── training/
│   ├── train.py                    # core training loop
│   └── eval.py                     # coherence, tone, response eval
├── logs/
│   └── metrics/
└── README.md
```




## Setup

1.  **Clone the repository (if not already done):**
    ```bash
    git clone <repository_url>
    cd natural_video_llm
    ```
2.  **Install dependencies:**
    ```bash
    pip install opencv-python torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
    pip install transformers
    ```

## Usage

### 1. Data Capture

To capture live webcam video and synchronize text input, run the following scripts in separate terminal sessions:

**Terminal 1 (Video Capture):**
```bash
python capture/webcam_video_logger.py
```

**Terminal 2 (Text Input):**
```bash
python capture/text_sync_logger.py
```

Press `q` in the video capture window to stop recording. Type `quit` in the text input terminal to stop.

This will generate video files and a `session.jsonl` file in the `natural_video_llm/capture/` directory.

### 2. Preprocessing

After capturing data, extract video latents and align text with video:

**Extract Video Latents:**
```bash
# Replace <your_video_file.mp4> with the actual video file generated by webcam_video_logger.py
python preprocessing/extract_video_latents.py
```
*Note: This script currently needs to be run for each video file. Future improvements could automate this.*

**Align Text and Video:**
```bash
python preprocessing/align_text_video.py
```
This will create `aligned_data.jsonl` in the `natural_video_llm/preprocessing/` directory.

### 3. Training

To train the multimodal LLM:

```bash
python training/train.py
```

This will save model checkpoints (e.g., `model_epoch_1.pt`) in the current directory.

### 4. Evaluation

To evaluate the trained model:

```bash
python training/eval.py
```

*Note: Ensure `model_checkpoint_path` in `eval.py` points to your trained model file.*


